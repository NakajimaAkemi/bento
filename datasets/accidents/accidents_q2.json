{
    "Input": [
        {
            "method": "load_dataset",
            "input": {                
                "nrows": 30000
            },
            "input_modin_ray": {
                "sep": ",",
                "nrows": 30000
            },
            "input_modin_dask": {
                "sep": ",",
                "nrows": 30000
            },
            "input_datatable": {
                "max_nrows": 30000
            },
            "input_polars": {
                "n_rows": 30000
            },            
            "input_spark": {
                "sep": ","
            },
            "input_vaex": {
                "lazy": true
            }
        },
        {
            "method": "force_execution",
            "input": {}
        }
    ],
    "Data Transformation": [
        {
            "method": "join",
            "input": {
                "other": "vehicles_df",
                "left_on": ["Accident_Index", "Vehicle_Reference"],
                "right_on": ["Accident_Index", "Vehicle_Reference"],
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "vehicles_df=pd.read_csv('datasets/accidents/data/Vehicles0514.csv', nrows=30000)"
                ]
            },            
            "input_modin_dask": {
                "other":"vehicles",
                "left_on": ["Accident_Index", "Vehicle_Reference"],
                "right_on": ["Accident_Index", "Vehicle_Reference"],
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "import modin.pandas as mpd",

                    "vehicles=pd.read_csv('datasets/accidents/data/Vehicles0514.csv',nrows=30000)",
                    "vehicles=mpd.DataFrame(vehicles)"
                ]
            },
            "input_modin_ray": {
                "other":"vehicles",
                "left_on": ["Accident_Index", "Vehicle_Reference"],
                "right_on": ["Accident_Index", "Vehicle_Reference"],
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "import modin.pandas as mpd",
                    
                    "vehicles=pd.read_csv('datasets/accidents/data/Vehicles0514.csv', nrows=30000)",
                    "vehicles=mpd.DataFrame(vehicles)"
                ]
            },
            "input_datatable": {
                "other": "vehicles_df",
                "left_on": ["Accident_Index", "Vehicle_Reference"],
                "right_on": ["Accident_Index", "Vehicle_Reference"],
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "vehicles_df=pd.read_csv('datasets/accidents/data/Vehicles0514.csv', nrows=30000)",
                    "import datatable as dt",
                    "vehicles_df=dt.Frame(vehicles_df)"                    
                ]
            },
            "input_polars": {
                "other": "vehicles_df",
                "left_on": ["Accident_Index", "Vehicle_Reference"],
                "right_on": ["Accident_Index", "Vehicle_Reference"],
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "vehicles_df=pd.read_csv('datasets/accidents/data/Vehicles0514.csv', nrows=30000)",
                    "import polars as pl",
                    "vehicles_df=pl.from_pandas(vehicles_df).lazy()"                    
                ]
            },
            "input_spark": {
                "other": "accident_df",
                "left_on": [
                    "Accident_Index"
                ],
                "right_on": [
                    "Accident_Index"
                ],
                "how": "inner",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "from pyspark.sql import DataFrame, SparkSession",
                    "sparkSession=SparkSession.builder.getOrCreate()",
                    "accident_df = sparkSession.read.csv('datasets/accidents/data/Accidents0514.csv', header=True, inferSchema=True)",
                    "accident_df.persist()"
                ]
            },
            "input_pyspark_pandas": {
                "other": "accident_df",
                "left_on": [
                    "Accident_Index"
                ],
                "right_on": [
                    "Accident_Index"
                ],
                "how": "inner",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pyspark.pandas as pd",
                    "accident_df = pd.read_csv('datasets/accidents/data/Accidents0514.csv')"
                ]
            },
            "input_vaex": {
                "other": "accident_df",
                "left_on": 
                    "Accident_Index",
                
                "right_on": 
                    "Accident_Index"
                ,
                "how": "inner",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import vaex",
                    "accident_df = vaex.read_csv('datasets/accidents/data/Accidents0514.csv')"
                ]
            }
        },
        {
            "method": "join",
            "input": {
                "other": "accident_df",
                "left_on": "Accident_Index",
                "right_on": "Accident_Index",
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "accident_df=pd.read_csv('datasets/accidents/data/Accidents0514.csv', nrows=30000)"
                ]
            },
            "input_modin_dask": {
                "other": "accident_df",
                "left_on": "Accident_Index",
                "right_on": "Accident_Index",
                "how": "left",
                "req_compile": [
                    "other"
                ],             
                "extra_commands": [
                    "import pandas as pd",
                    "import modin.pandas as mpd",
                    "accident_df=pd.read_csv('datasets/accidents/data/Accidents0514.csv', nrows=30000)",
                    "accident_df=mpd.DataFrame(accident_df)"
                ]
            },
            "input_modin_ray": {
                "other": "accident_df",
                "left_on": "Accident_Index",
                "right_on": "Accident_Index",
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "import modin.pandas as mpd",
                    "accident_df=pd.read_csv('datasets/accidents/data/Accidents0514.csv', nrows=30000)",
                    "accident_df=mpd.DataFrame(accident_df)"
                ]
            },
            "input_datatable": {
                "other": "accident_df",
                "left_on": "Accident_Index",
                "right_on": "Accident_Index",
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "accident_df=pd.read_csv('datasets/accidents/data/Accidents0514.csv', nrows=30000)",
                    "import datatable as dt",
                    "accident_df=dt.Frame(accident_df)"                    
                ]
            },
            "input_polars": {
                "other": "accident_df",
                "left_on": "Accident_Index",
                "right_on": "Accident_Index",
                "how": "left",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pandas as pd",
                    "accident_df=pd.read_csv('datasets/accidents/data/Accidents0514.csv', nrows=30000)",
                    "import polars as pl",
                    "accident_df=pl.from_pandas(accident_df).lazy()"                    
                ]
            },
            "input_spark": {
                "other": "vehicle_df",
                "left_on": [
                    "Accident_Index"
                ],
                "right_on": [
                    "Accident_Index"
                ],
                "how": "inner",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "from pyspark.sql import DataFrame, SparkSession",
                    "sparkSession=SparkSession.builder.getOrCreate()",
                    "vehicle_df = sparkSession.read.csv('datasets/accidents/data/Vehicles0514.csv', header=True, inferSchema=True)",
                    "vehicle_df.persist()"
                ]
            },
            "input_pyspark_pandas": {
                "other": "vehicle_df",
                "left_on": [
                    "Accident_Index"
                ],
                "right_on": [
                    "Accident_Index"
                ],
                "how": "inner",
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import pyspark.pandas as pd",
                    "vehicle_df = pd.read_csv('datasets/accidents/data/Vehicles0514.csv')"
                ]
            },
            "input_vaex": {
                "other": "vehicle_df",
                "left_on": 
                    "Accident_Index",
                
                "right_on": 
                    "Accident_Index"
                ,
                "how": "inner",
                "rprefix": "V_",
                "allow_duplication": true,
                "req_compile": [
                    "other"
                ],
                "extra_commands": [
                    "import vaex",
                    "vehicle_df = vaex.read_csv('datasets/accidents/data/Vehicles0514.csv')"
                ]
            }
        },        
        {
            "method": "delete_columns",
            "input": {
                "columns": ["LSOA_of_Accident_Location"]
            }
        },
        {
            "method": "calc_column",
            "input": {
                "col_name": "Month",
                "columns": ["Date"],
                "f": "lambda date_str: int(str(date_str).split('/')[1])"
            }
        },   
        {
            "method": "calc_column",
            "input": {
                "col_name": "Hour",
                "columns": ["Time"],
                "f": "lambda s: str(s.tolist())[2:4]"    
            },
            "input_polars": {
                "col_name": "Hour",
                "columns": ["Time"],
                "f": "lambda s: 'IS A SLICE' if type(s) == pl.DataFrame.slice else s['Time'][0:2]"
            },
            "input_vaex": {
                "col_name": "Hour",
                "columns": [ "Time"],
                "f": "lambda s: s[0:2]"    
            },
            "input_spark": {
                "col_name": "Hour",
                "columns": [ "Time"],
                "f": "lambda s: str(s)[11:13]"    
            },
            "input_pyspark_pandas": {
                "col_name": "Hour",
                "columns": [ "Time"],
                "f": "lambda s: str(s)[18:20]"   
            }
        },
        {
            "method":"loc",
            "input": {
                "column_list": [
                    "Journey_Purpose_of_Driver", 
                    "Driver_Home_Area_Type",
                    "Age_Band_of_Driver",
                    "Sex_of_Driver", 
                    "Age_of_Driver"
                ],
                "row_condition": "self.df_.Sex_of_Driver != -1"
            },
            "input_polars": {
                "column_list": [
                    "Journey_Purpose_of_Driver", 
                    "Driver_Home_Area_Type",
                    "Age_Band_of_Driver",
                    "Sex_of_Driver", 
                    "Age_of_Driver"
                ],
                "row_condition": "self.df_.select('Sex_of_Driver') != -1",                
                "extra_commands": [
                    "import polars as pl"
                ]
            },
            "input_datatable": {
                "column_list": [
                    "Journey_Purpose_of_Driver", 
                    "Driver_Home_Area_Type",
                    "Age_Band_of_Driver",
                    "Sex_of_Driver", 
                    "Age_of_Driver"
                ],
                "row_condition": "(dt.f.Sex_of_Driver != -1)"                
            },
            "input_modin_dask": {
                "column_list": [
                    "Journey_Purpose_of_Driver", 
                    "Driver_Home_Area_Type",
                    "Age_Band_of_Driver",
                    "Sex_of_Driver", 
                    "Age_of_Driver"
                ],
                "row_condition":"(self.df_.Sex_of_Driver != '-1') & (self.df_.Location_Easting_OSGR.notna()) & (self.df_.Location_Northing_OSGR.notna()) & (self.df_.Longitude.notna()) & (self.df_.Latitude.notna()) & (self.df_.Time!= None)"  
            },
            "input_modin_ray": {
                "column_list": [
                    "Journey_Purpose_of_Driver", 
                    "Driver_Home_Area_Type",
                    "Age_Band_of_Driver",
                    "Sex_of_Driver", 
                    "Age_of_Driver"    
                ],
                "row_condition":"(self.df_.Sex_of_Driver != -1.0) & (self.df_.Location_Easting_OSGR.notna()) & (self.df_.Location_Northing_OSGR.notna()) & (self.df_.Longitude.notna()) & (self.df_.Latitude.notna()) & (self.df_.Time!= None)" 
            },
            "input_spark":{
                "column_list":["Journey_Purpose_of_Driver", "Sex_of_Driver", "Age_of_Driver","Age_Band_of_Driver","Driver_Home_Area_Type"],
                "row_condition":"Sex_of_Driver != -1"  
            },
            "input_pyspark_pandas":{
                "column_list":["Journey_Purpose_of_Driver", "Sex_of_Driver", "Age_of_Driver","Age_Band_of_Driver","Driver_Home_Area_Type"],
                "row_condition":"Sex_of_Driver != -1"  
            },
            "input_vaex":{
                "column_list":["Journey_Purpose_of_Driver", "Sex_of_Driver", "Age_of_Driver","Age_Band_of_Driver","Driver_Home_Area_Type"],
                "row_condition":"self.df_.Sex_of_Driver != -1"  
            }
        },
        {
            "method": "force_execution",
            "input": {}
        }
    ],    
    "Data Cleaning": [
        {
            "method": "replace",
            "input": {
                "columns": ["Journey_Purpose_of_Driver"],
                "to_replace": [1, 2, 3, 4, 5, 6, 15],
                "value": [
                    "Journey as part of work", 
                    "Commuting to/from work", 
                    "Taking pupil to/from school", 
                    "Pupil riding to/from school", 
                    "Other", 
                    "Not known", 
                    "Not known/Other"
                ],
                "regex": false
            },
            "input_modin_dask": {
                "columns": ["Journey_Purpose_of_Driver"],
                "to_replace": [1, 2, 3, 4, 5, 6, 15],
                "value": [
                    "Journey as part of work", 
                    "Commuting to/from work", 
                    "Taking pupil to/from school", 
                    "Pupil riding to/from school", 
                    "Other", 
                    "Not known", 
                    "Not known/Other"
                ],
                "regex": false
            },
            "input_modin_ray": {
                "columns": ["Journey_Purpose_of_Driver"],
                "to_replace": [1, 2, 3, 4, 5, 6, 15],
                "value": [
                    "Journey as part of work", 
                    "Commuting to/from work", 
                    "Taking pupil to/from school", 
                    "Pupil riding to/from school", 
                    "Other", 
                    "Not known", 
                    "Not known/Other"
                ],
                "regex": false
            }
        },
        {
            "method": "replace",
            "input": {
                "columns": ["Age_Band_of_Driver"],
                "to_replace": [
                    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11
                    ],
                "value": [
                    "0 - 5", "6 - 10", "11 - 15", "16 - 20", "21 - 25", "26 - 35", "36 - 45", "46 - 55", "56 - 65", "66 - 75", "Over 75"
                ],
                "regex": false
            },
            "input_modin_dask": {
                "columns": ["Age_Band_of_Driver"],
                "to_replace": [1, 2, 3, 4, 5, 6,7, 8, 9, 10, 11],
                "value": ["0 - 5", "6 - 10", "11 - 15", "16 - 20", "21 - 25", "26 - 35", "36 - 45", "46 - 55", "56 - 65", "66 - 75", "Over 75"],
                "regex": false
            },
            "input_modin_ray": {
                "columns": ["Age_Band_of_Driver"],
                "to_replace": [1, 2, 3, 4,5, 6, 7, 8, 9, 10, 11],
                "value": ["0 - 5", "6 - 10", "11 - 15", "16 - 20", "21 - 25", "26 - 35", "36 - 45", "46 - 55", "56 - 65", "66 - 75", "Over 75"],
                "regex": false
            },
            "input_spark": {
                "columns": ["Age_Band_of_Driver"],
                "to_replace": [-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
                "value": ["NaN", "0 - 5", "6 - 10", "11 - 15", "16 - 20", "21 - 25", "26 - 35", "36 - 45", "46 - 55", "56 - 65", "66 - 75", "Over 75"],
                "regex": false
            },
            "input_pyspark_pandas": {
                "columns": ["Age_Band_of_Driver"],
                "to_replace": [-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
                "value": ["NaN", "0 - 5", "6 - 10", "11 - 15", "16 - 20", "21 - 25", "26 - 35", "36 - 45", "46 - 55", "56 - 65", "66 - 75", "Over 75"],
                "regex": false
            },
            "input_vaex": {
                "columns": ["Age_Band_of_Driver"],
                "to_replace": [-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
                "value": ["NaN", "0 - 5", "6 - 10", "11 - 15", "16 - 20", "21 - 25", "26 - 35", "36 - 45", "46 - 55", "56 - 65", "66 - 75", "Over 75"],
                "regex": false
            }                
        },        
        {
            "method": "replace",
            "input": {
                "columns": ["Driver_Home_Area_Type"],
                "to_replace": [
                    1, 2, 3, -1
                ],
                "value": [
                    "Urban Area", "Small Town", "Rural", "-1"
                ],
                "regex": false
            },
            "input_modin_dask": {
                "columns": ["Driver_Home_Area_Type"],
                "to_replace": [1, "2.0", "3.0"],
                "value": ["Urban Area", "Small Town", "Rural"],
                "regex": false
            },
            "input_modin_ray": {
                "columns": ["Driver_Home_Area_Type"],
                "to_replace": [1, 2, 3,-1],
                "value": ["Urban Area", "Small Town", "Rural", "-1"],
                "regex": false
            }
        },
        {
            "method": "force_execution",
            "input": {}
        }
    ],
    "EDA": [

    ],
    "Output": [
        {
            "method": "to_csv",
            "input": {
                "extra_commands": [
                    "import os",
                    "if not os.path.exists('pipeline_output'): os.makedirs('pipeline_output')"
                ]
            }
        }
    ]
}